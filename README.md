# ğŸŒ± Data Engineering Project 

In a world where food security is paramount, this project aims to enhance agricultural productivity and sustainability through advanced data engineering techniques.

This project is a scalable, containerized data engineering platform for **sensor data ingestion**, **stream processing**, and **alert detection**, built with **Apache Kafka**, **Apache Spark** and **Grafana**, with **AWS S3** and **PostgreSQL** as storage. **Docker** and **Docker compose** are used for conterinerization and orchestration.

---

## ğŸ“Œ Overview

This project simulates and ingests IoT sensor data into Kafka, processes it via Spark Streaming jobs through bronze-silver-gold stages, and detects anomalies. It includes:
- **Data Pipeline**: Kafka â†’ Spark â†’ S3
- **Alerting Pipeline**: Spark jobs identify and forward anomalies
- **Monitoring**: Grafana dashboards fed from processed data
- **Simulation**: A `sensor-simulator` service generates test data

---

## ğŸ“Š Architecture

![Architecture Diagram](images/architecture.png)

---

## ğŸ“‚ Project Structure (Simplified)

```
alerting-pipeline/
  â””â”€â”€ spark/                               # Contains Core logic for alerts
data-pipeline/
  â””â”€â”€ spark/
      â”œâ”€â”€ src/main/scala/ingestion/        # Spark ingestion logic
      |    â””â”€â”€ KafkaIngest.scala
      â””â”€â”€ src/main/scala/processing/       # Contains data processing / grafana Spark jobs
          â”œâ”€â”€ BronzeJob.scala
          â”œâ”€â”€ SilverJob.scala
          â”œâ”€â”€ GoldJob.scala
          â””â”€â”€ GrafanaExportJob.scala
sensor-simulator/                          # Core data simulation logic
  â””â”€â”€ src/main/scala/
      â””â”€â”€ Producer.scala
scripts/                                   # Helper scripts
  â”œâ”€â”€ master-workflow.sh
  â””â”€â”€ ...
docker/                                    # Dockerfiles and Compose files             
  â”œâ”€â”€ docker-compose.yml
  â””â”€â”€ docker-compose.pipeline.yml
docker-compose.unified.yml                 # Unified Main Docker Compose for all services
```

---

## ğŸ” Data Pipeline Breakdown

### ğŸŸ  Ingestion (Bronze Layer)
- **Script**: `KafkaIngest.scala`
- **Function**: Reads raw sensor data from Kafka and stores it in the **Bronze S3 layer**.

### âšª Transformation (Silver Layer)
- **Script**: `SilverJob.scala`
- **Function**: Cleans and enriches Bronze data, stores in **Silver S3 layer**.

### ğŸŸ¡ Aggregation (Gold Layer + Alerts)
- **Script**: `GoldJob.scala`
- **Function**:
  - Aggregates Silver data to generate key metrics.
  - Exports data for Grafana dashboards.

---

## ğŸš¨ Alert Pipeline

- **Input**: Kafka topic with sensor data
- **Detection Logic**: Encoded in `KafkaAlertingPipeline.scala`
- **Outputs**:
  - Alert messages to an email client

---

## ğŸ“¦ Services

| Service      | Description                      |
|--------------|----------------------------------|
| Kafka        | Data broker and streaming   |
| Spark        | Streaming / Batch job executor           |
| Zookeeper   | Kafka cluster management         |
| Grafana      | Dashboard visualization          |
| Sensor Sim   | Data generation (simulator)      |
| AWS S3     | Data lake storage      |
| PostgreSQL   | Analytics storage                |
| Alerting Pipeline | Anomaly detection and alerting |

---

## âš™ï¸ Setup

### 1. âœ… Prerequisites
- Docker + Docker Compose
- AWS credentials (for S3 access)
- JDK 8+ and Scala

### 2. ğŸ“ Configure `.env`

Copy and edit:
```bash
cp .env.example .env
```

Fill in your AWS, Kafka/Spark and SMTP email variables.

### 3. ğŸš€ Run the Data Pipeline

The `./scripts/master-workflow.sh` script orchestrates the entire pipeline, including building the docker images, starting services and running the jobs.

```bash
# Run master workflow script
./scipts/master-workflow.sh
```

---

## ğŸ“ˆ Monitoring & Visualization

### Grafana Setup

- Runs at: [http://localhost:3000](http://localhost:3000)
- Default credentials: `admin / admin`
- Dashboards auto-loaded from `grafana-config/dashboard-model.json`

### Alert setup

- Configure SMTP variables in `.env` to receive email alerts.
- Alerts are sent to the configured email when thresholds are breached.
- Alert detection logic is in `KafkaAlertingPipeline.scala`.

---

## â˜ï¸ Data Lake

- Data is written to `s3://inde-aws-datalake/` via AWS credentials.
- You can access structured layers:
  - `/bronze/`
  - `/silver/`
  - `/gold/`
- Supports real AWS S3 bucket

---


## ğŸ”§ Useful Scripts

| Script                         | Purpose                            |
|--------------------------------|------------------------------------|
| `start-services.sh`           | Bootstraps all core services       |
| `run-pipeline.sh`             | Runs full Spark-based pipeline     |
| `build-containers.sh`       | Builds all Docker containers       |
| `build-and-run-spark-ingest.sh` | Build + run Spark ingestion       |
| `run_pipeline_scheduled.sh`   | Runs pipeline periodically         |
| `log-messages.sh`             | Reads logs from Kafka              |
| `produce-test-messages.sh` | Produces test message to Kafka   |

---

## ğŸ§‘â€ğŸ’»  Development Team

CÃ©dric Damais \
Yacine Benihaddadene \
Gabriel Calvente \
LÃ©on Ayral

---

## ğŸ“„ License

MIT License
