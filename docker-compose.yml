version: '3.8'


# ================================
# NETWORKS
# ================================

networks:
  iot-pipeline-net:
    driver: bridge


# ================================
# VOLUMES
# ================================

volumes:
  kafka-data:
  pgdata:
  grafana_data:
  iot-checkpoints:
  spark-logs:
    driver: local
  pipeline-logs:
    driver: local 

services:
  # ================================
  # INFRASTRUCTURE SERVICES
  # ================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.2.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - iot-pipeline-net
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.2.1
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_LISTENERS: "INTERNAL://0.0.0.0:9093,EXTERNAL://0.0.0.0:9092"
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka:9093,EXTERNAL://localhost:9092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      #KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - iot-pipeline-net
    volumes:
      - kafka-data:/var/lib/kafka
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5


  kafka-setup:
    image: confluentinc/cp-kafka:7.2.1
    container_name: kafka-setup-iot
    depends_on:
      - kafka
    networks:
      - iot-pipeline-net
    command: >
      bash -c "
        echo 'Waiting for Kafka to be ready...'
        while ! kafka-topics --list --bootstrap-server kafka:29092 &>/dev/null; do
          sleep 1
        done
        echo 'Creating topic: iot-sensor-data'
        kafka-topics --create --if-not-exists --topic iot-sensor-data --bootstrap-server kafka:29092 --partitions 1 --replication-factor 1
        echo 'Topic created successfully'
      "
    restart: "no"


  # ================================
  # DATABASE & MONITORING
  # ================================

  postgres:
    image: postgres:14
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: grafana
      POSTGRES_PASSWORD: grafana
      POSTGRES_DB: grafana_db
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    networks:
      - iot-pipeline-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U grafana"]
      interval: 10s
      timeout: 5s
      retries: 5


  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - iot-pipeline-net


  # ================================
  # SPARK CLUSTER
  # ================================

  spark-master:
    image: bitnami/spark:3.5.1
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-eu-east-1}
    ports:
      - "8081:8080"
      - "7077:7077"
    networks:
      - iot-pipeline-net
    volumes:
      - ./data-pipeline/spark:/opt/spark-apps
      - ./storage:/opt/storage

  spark-worker-1:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-eu-east-1}
    depends_on:
      spark-master:
        condition: service_started
    networks:
      - iot-pipeline-net
    volumes:
      - ./data-pipeline/spark:/opt/spark-apps
      - ./storage:/opt/storage

  spark-worker-2:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-eu-east-1}
    depends_on:
      spark-master:
        condition: service_started
    networks:
      - iot-pipeline-net
    volumes:
      - ./data-pipeline/spark:/opt/spark-apps
      - ./storage:/opt/storage


  # ================================
  # DATA GENERATION
  # ================================

  iot-producer:
    build:
      context: ./sensor-simulator/
      dockerfile: Dockerfile
    container_name: iot-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9093
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_DEFAULT_REGION:-us-east-1}
      - TOPIC=iot-sensor-data
    networks:
      - iot-pipeline-net
    profiles:
      - producer



  # ================================
  # KAFKA INGESTION
  # ================================
  kafka-ingestion:
    build:
      context: ./data-pipeline/spark/
      dockerfile: ../../docker/Dockerfile.unified
    container_name: kafka-ingestion
    depends_on:
      spark-master:
        condition: service_started
      kafka:
        condition: service_healthy
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-eu-west-3}
      - SPARK_APPLICATION_MAIN_CLASS=ingestion.KafkaS3DataLakePipeline
      - SPARK_DRIVER_MEMORY=2g
      - SPARK_EXECUTOR_MEMORY=2g
      - PROCESS_DATE=${PROCESS_DATE}
    volumes:
      - ./logs:/opt/spark-apps/logs
    networks:
      - iot-pipeline-net
    profiles:
      - ingestion



# ================================
  # DATA PROCESSING JOBS
# ================================
  bronze-job:
    build:
      context: ./data-pipeline/spark
      dockerfile: ../../docker/Dockerfile.unified
    container_name: bronze-job
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-eu-west-3}
      - SPARK_APPLICATION_MAIN_CLASS=processing.BronzeJob
      - SPARK_DRIVER_MEMORY=2g
      - SPARK_EXECUTOR_MEMORY=2g
      - PROCESS_DATE=${PROCESS_DATE}
    volumes:
      - ./logs:/opt/spark-apps/logs
    networks:
      - iot-pipeline-net
    profiles:
      - processing

  silver-job:
    build:
      context: data-pipeline/spark
      dockerfile: ../../docker/Dockerfile.unified
    container_name: silver-job
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-eu-west-3}
      - SPARK_APPLICATION_MAIN_CLASS=processing.SilverJob
      - SPARK_DRIVER_MEMORY=2g
      - SPARK_EXECUTOR_MEMORY=2g
      - PROCESS_DATE=${PROCESS_DATE}
    depends_on:
      - bronze-job
    volumes:
      - ./logs:/opt/spark-apps/logs
    networks:
      - iot-pipeline-net
    profiles:
      - processing

  gold-job:
    build:
      context: data-pipeline/spark
      dockerfile: ../../docker/Dockerfile.unified
    container_name: gold-job
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-eu-west-3}
      - SPARK_APPLICATION_MAIN_CLASS=processing.GoldJob
      - SPARK_DRIVER_MEMORY=2g
      - SPARK_EXECUTOR_MEMORY=2g
      - PROCESS_DATE=${PROCESS_DATE}
    depends_on:
      - silver-job
    volumes:
      - ./logs:/opt/spark-apps/logs
    networks:
      - iot-pipeline-net
    profiles:
      - processing

  grafana-export:
    build:
      context: data-pipeline/spark
      dockerfile: ../../docker/Dockerfile.unified
    container_name: grafana-export
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-eu-west-3}
      - SPARK_APPLICATION_MAIN_CLASS=processing.GrafanaExportJob
      - SPARK_DRIVER_MEMORY=2g
      - SPARK_EXECUTOR_MEMORY=2g
      - PROCESS_DATE=${PROCESS_DATE}
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=grafana_db
      - POSTGRES_USER=grafana
      - POSTGRES_PASSWORD=grafana
    depends_on:
      - gold-job
      - postgres
    volumes:
      - ./logs:/opt/spark-apps/logs
    networks:
      - iot-pipeline-net
    profiles:
      - processing


# ================================
# ALERTING PIPELINE
# ================================


  iot-alerting-pipeline:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.alert

    container_name: iot-alerting-consumer
    depends_on:
      - kafka
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC: iot-sensor-data
      KAFKA_CONSUMER_GROUP_ID: iot-alerting-consumer-compose
      KAFKA_STARTING_OFFSETS: earliest
      SPARK_APP_NAME: IoT-Kafka-Alerting-Pipeline-Compose
      SPARK_CHECKPOINT_LOCATION: /tmp/kafka-alerting-checkpoints/
      SPARK_PROCESSING_TIME_SECONDS: 5
      SPARK_LOG_LEVEL: WARN
      ALERT_RECIPIENT_EMAIL: admin@company.com
      SMTP_HOST: ${SMTP_HOST:-sandbox.smtp.mailtrap.io}
      SMTP_PORT: ${SMTP_PORT:-587}
      SMTP_USER: ${SMTP_USER:-}
      SMTP_PASSWORD: ${SMTP_PASSWORD:-}
      EMAIL_FROM_ADDRESS: ${EMAIL_FROM:-iot-alerts@company.com}
      EMAIL_SUBJECT_PREFIX: "[IoT Alert - Compose]"
      JAVA_OPTS: "-XX:+UseContainerSupport -XX:MaxRAMPercentage=80.0 -Djdk.lang.processReaperUseDefaultStackSize=true"
    volumes:
      - iot-checkpoints:/tmp/kafka-alerting-checkpoints
    networks:
      - iot-pipeline-net