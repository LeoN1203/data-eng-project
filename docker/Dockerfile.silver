FROM bitnami/spark:3.5.1

# Install required packages
USER root
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /opt/spark-apps

# Copy the assembled JAR
COPY target/scala-2.12/data-pipeline-scala-assembly-0.1.0-SNAPSHOT.jar /opt/spark-apps/data-pipeline.jar

# Silver job specific configurations (more memory for data quality processing)
ENV SPARK_MASTER="local[*]"
ENV SPARK_DRIVER_MEMORY="3g"
ENV SPARK_EXECUTOR_MEMORY="3g"
ENV SPARK_EXECUTOR_CORES="2"

# Enable adaptive query execution for better performance
ENV SPARK_SQL_ADAPTIVE_ENABLED="true"
ENV SPARK_SQL_ADAPTIVE_COALESCE_PARTITIONS_ENABLED="true"

# AWS S3 packages
ENV SPARK_PACKAGES="org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.376"

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:4040 || exit 1

# Switch back to spark user
USER 1001

# Entry point for Silver job
ENTRYPOINT ["spark-submit", \
  "--class", "processing.SilverJob", \
  "--master", "local[*]", \
  "--driver-memory", "3g", \
  "--executor-memory", "3g", \
  "--packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.376", \
  "--conf", "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem", \
  "--conf", "spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider", \
  "--conf", "spark.sql.adaptive.enabled=true", \
  "--conf", "spark.sql.adaptive.coalescePartitions.enabled=true", \
  "/opt/spark-apps/data-pipeline.jar"] 