FROM bitnami/spark:3.5.1

# Install required packages for S3 connectivity
USER root
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /opt/spark-apps

# Copy the assembled JAR
COPY target/scala-2.12/data-pipeline-scala-assembly-0.1.0-SNAPSHOT.jar /opt/spark-apps/data-pipeline.jar

# Set environment variables
ENV SPARK_APPLICATION_JAR_LOCATION="/opt/spark-apps/data-pipeline.jar"
ENV SPARK_APPLICATION_MAIN_CLASS="processing.BronzeJob"
ENV SPARK_APPLICATION_ARGS=""

# Bronze job specific configurations
ENV SPARK_MASTER="local[*]"
ENV SPARK_DRIVER_MEMORY="2g"
ENV SPARK_EXECUTOR_MEMORY="2g"
ENV SPARK_EXECUTOR_CORES="2"

# AWS S3 packages
ENV SPARK_PACKAGES="org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.376,io.delta:delta-core_2.12:2.4.0"

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:4040 || exit 1

# Switch back to spark user
USER 1001

# Entry point for Bronze job
ENTRYPOINT ["spark-submit", \
  "--class", "processing.BronzeJob", \
  "--master", "local[*]", \
  "--driver-memory", "2g", \
  "--executor-memory", "2g", \
  "--packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.376,io.delta:delta-core_2.12:2.4.0", \
  "--conf", "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem", \
  "--conf", "spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider", \
  "--conf", "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension", \
  "--conf", "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog", \
  "/opt/spark-apps/data-pipeline.jar"] 